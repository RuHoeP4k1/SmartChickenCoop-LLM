import ollama

# ============================================================
# âš™ï¸ Model setup
model_name = "qwen:3-4b"  # Name as installed in Ollama

print(f"ğŸš€ Loading model: {model_name}")

# ============================================================
# ğŸ—£ï¸ Prompt
prompt = "Why do cows produce milk?"

print("ğŸ§© Generating response...")

# Generate using Ollama
response = ollama.generate(
    model=model_name,
    prompt=prompt,
    options={
        "temperature": 0.7,
        "top_p": 0.9,
        "num_predict": 200
    }
)

# ============================================================
# ğŸ“¤ Print result
print("\n=============================")
print(f"ğŸ§  {model_name} says:\n")
print(response["response"])
print("=============================\n")
